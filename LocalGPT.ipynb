{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/boloson/localGPT/blob/main/LocalGPT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oB7QTo2_GTv3",
        "outputId": "b66843d5-7f4e-42a9-aaf6-0b801eefc544"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Aug  6 23:07:22 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   40C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNZ_1mYfGsQn",
        "outputId": "4b5101b2-255f-4d6c-f736-07e0d7def799"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tensorrt\n",
            "  Downloading tensorrt-8.6.1.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: tensorrt\n",
            "  Building wheel for tensorrt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tensorrt: filename=tensorrt-8.6.1-py2.py3-none-any.whl size=16972 sha256=63b78a6fc6b3633420689939390ea315f691c84edfe297e42bee28efc298f9d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/6d/29/56/abdffd4c604f255b5254bef3f1c598ab7811ea020540599438\n",
            "Successfully built tensorrt\n",
            "Installing collected packages: tensorrt\n",
            "Successfully installed tensorrt-8.6.1\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade tensorrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MRPCpzIzWvlP",
        "outputId": "6cadb257-182e-4785-abdd-c24c84974d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "8.6.1\n"
          ]
        }
      ],
      "source": [
        "import tensorrt\n",
        "print(tensorrt.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "YVjFe2MbbyNc",
        "outputId": "1b580f60-df0b-467a-86dc-284e73eee444"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://cfhqkjji7j9-496ff2e9c6d22116-5110-colab.googleusercontent.com/\n",
            "https://jrffar8emor-496ff2e9c6d22116-5111-colab.googleusercontent.com/\n"
          ]
        }
      ],
      "source": [
        "from google.colab.output import eval_js\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5110)\"))\n",
        "print(eval_js(\"google.colab.kernel.proxyPort(5111)\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w4FJA0cLG1EW"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/PromtEngineer/localGPT.git\n",
        "%cd localGPT\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "2dMVt_ZvJI55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6318d84b-6f0d-4461-cc7c-7a4808b26780"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-08-07 01:33:02,942 - INFO - ingest.py:121 - Loading documents from /content/localGPT/SOURCE_DOCUMENTS\n",
            "2023-08-07 01:33:02,965 - INFO - ingest.py:34 - Loading document batch\n",
            "2023-08-07 01:33:09,207 - INFO - ingest.py:130 - Loaded 1 documents from /content/localGPT/SOURCE_DOCUMENTS\n",
            "2023-08-07 01:33:09,207 - INFO - ingest.py:131 - Split into 72 chunks of text\n",
            "2023-08-07 01:33:12,372 - INFO - SentenceTransformer.py:66 - Load pretrained SentenceTransformer: hkunlp/instructor-large\n",
            "Downloading (…)c7233/.gitattributes: 100% 1.48k/1.48k [00:00<00:00, 7.76MB/s]\n",
            "Downloading (…)_Pooling/config.json: 100% 270/270 [00:00<00:00, 1.37MB/s]\n",
            "Downloading (…)/2_Dense/config.json: 100% 116/116 [00:00<00:00, 661kB/s]\n",
            "Downloading pytorch_model.bin: 100% 3.15M/3.15M [00:00<00:00, 25.8MB/s]\n",
            "Downloading (…)9fb15c7233/README.md: 100% 66.3k/66.3k [00:00<00:00, 33.7MB/s]\n",
            "Downloading (…)b15c7233/config.json: 100% 1.53k/1.53k [00:00<00:00, 8.53MB/s]\n",
            "Downloading (…)ce_transformers.json: 100% 122/122 [00:00<00:00, 646kB/s]\n",
            "Downloading pytorch_model.bin: 100% 1.34G/1.34G [00:35<00:00, 37.9MB/s]\n",
            "Downloading (…)nce_bert_config.json: 100% 53.0/53.0 [00:00<00:00, 312kB/s]\n",
            "Downloading (…)cial_tokens_map.json: 100% 2.20k/2.20k [00:00<00:00, 15.0MB/s]\n",
            "Downloading spiece.model: 100% 792k/792k [00:00<00:00, 28.2MB/s]\n",
            "Downloading (…)c7233/tokenizer.json: 100% 2.42M/2.42M [00:00<00:00, 27.3MB/s]\n",
            "Downloading (…)okenizer_config.json: 100% 2.41k/2.41k [00:00<00:00, 15.3MB/s]\n",
            "Downloading (…)15c7233/modules.json: 100% 461/461 [00:00<00:00, 3.36MB/s]\n",
            "load INSTRUCTOR_Transformer\n",
            "2023-08-07 01:33:51,825 - INFO - instantiator.py:21 - Created a temporary directory at /tmp/tmpt0ha2wmf\n",
            "2023-08-07 01:33:51,826 - INFO - instantiator.py:76 - Writing /tmp/tmpt0ha2wmf/_remote_module_non_scriptable.py\n",
            "2023-08-07 01:33:53.171542: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "max_seq_length  512\n",
            "2023-08-07 01:33:58,222 - INFO - __init__.py:88 - Running Chroma using direct local API.\n",
            "2023-08-07 01:33:58,236 - WARNING - __init__.py:43 - Using embedded DuckDB with persistence: data will be stored in: /content/localGPT/DB\n",
            "2023-08-07 01:33:58,242 - INFO - ctypes.py:22 - Successfully imported ClickHouse Connect C data optimizations\n",
            "2023-08-07 01:33:58,254 - INFO - json_impl.py:45 - Using python library for writing JSON byte strings\n",
            "2023-08-07 01:33:58,588 - INFO - duckdb.py:454 - No existing DB found in /content/localGPT/DB, skipping load\n",
            "2023-08-07 01:33:58,589 - INFO - duckdb.py:466 - No existing DB found in /content/localGPT/DB, skipping load\n",
            "2023-08-07 01:34:14,111 - INFO - duckdb.py:414 - Persisting DB to disk, putting it in the save folder: /content/localGPT/DB\n",
            "2023-08-07 01:34:14,135 - INFO - duckdb.py:414 - Persisting DB to disk, putting it in the save folder: /content/localGPT/DB\n"
          ]
        }
      ],
      "source": [
        "!python ingest.py --device_type cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-116iT4FJMOR"
      },
      "outputs": [],
      "source": [
        "!python run_localGPT.py --device_type cuda"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "YbD_aiVPICXV",
        "outputId": "fe56a21b-8c51-4ad6-ac12-bb3773c38641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " * Serving Flask app 'localGPTUI'\n",
            " * Debug mode: off\n",
            "\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5111\n",
            "\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "2023-08-07 02:37:26.363773: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "load INSTRUCTOR_Transformer\n",
            "max_seq_length  512\n",
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /content/localGPT/DB\n",
            "WARNING:accelerate.utils.modeling:The safetensors archive passed at /root/.cache/huggingface/hub/models--TheBloke--WizardLM-7B-uncensored-GPTQ/snapshots/dcb3400039f15cff76b43a4921c59d47c5fc2252/WizardLM-7B-uncensored-GPTQ-4bit-128g.compat.no-act-order.safetensors does not contain metadata. Make sure to save your model with the `save_pretrained` method. Defaulting to 'pt' metadata.\n",
            "WARNING:auto_gptq.nn_modules.fused_llama_mlp:skip module injection for FusedLlamaMLPForQuantizedModel not support integrate without triton yet.\n",
            "Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n",
            "pip install xformers.\n",
            "The model 'LlamaGPTQForCausalLM' is not supported for text-generation. Supported models are ['BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'CamembertForCausalLM', 'CodeGenForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'Data2VecTextForCausalLM', 'ElectraForCausalLM', 'ErnieForCausalLM', 'FalconForCausalLM', 'GitForCausalLM', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GPTJForCausalLM', 'LlamaForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegaForCausalLM', 'MegatronBertForCausalLM', 'MusicgenForCausalLM', 'MvpForCausalLM', 'OpenLlamaForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'QDQBertLMHeadModel', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'Speech2Text2ForCausalLM', 'TransfoXLLMHeadModel', 'TrOCRForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMProphetNetForCausalLM', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'XmodForCausalLM'].\n",
            " * Serving Flask app 'run_localGPT_API'\n",
            " * Debug mode: off\n",
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5110\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "127.0.0.1 - - [07/Aug/2023 02:38:42] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:38:42] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:38:42] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:38:43] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:38:44] \"GET /static/social_icons/favicon.png HTTP/1.1\" 200 -\n",
            "User Prompt: how long is a term for us president?\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:38:53] \"POST /api/prompt_route HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:38:53] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:38:53] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:38:54] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:38:54] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:39:34] \"GET /api/delete_source HTTP/1.1\" 200 -\n",
            "varian_elsd_technical_document.pdf\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:39:34] \"POST /api/save_document HTTP/1.1\" 200 -\n",
            "200\n",
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /content/localGPT/DB\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:40:32] \"GET /api/run_ingest HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:40:32] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:40:33] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:40:33] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:40:33] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "User Prompt: how to clean nebulizer\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:41:01] \"POST /api/prompt_route HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:41:01] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:41:02] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:41:02] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:41:02] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "Agilent 5977B Series MSD Operating Manual.pdf\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:44:26] \"POST /api/save_document HTTP/1.1\" 200 -\n",
            "200\n",
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /content/localGPT/DB\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:45:43] \"GET /api/run_ingest HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:45:43] \"POST / HTTP/1.1\" 200 -\n",
            "Agilent 5977B Series MSD Operating Manual.pdf\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:46:30] \"POST /api/save_document HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:28] \"GET / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:28] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:28] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:29] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:30] \"GET /static/social_icons/favicon.png HTTP/1.1\" 200 -\n",
            "User Prompt: how often do you clean the EI source?\n",
            "ERROR:run_localGPT_API:Exception on /api/prompt_route [POST]\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 2529, in wsgi_app\n",
            "    response = self.full_dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1825, in full_dispatch_request\n",
            "    rv = self.handle_user_exception(e)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1823, in full_dispatch_request\n",
            "    rv = self.dispatch_request()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/flask/app.py\", line 1799, in dispatch_request\n",
            "    return self.ensure_sync(self.view_functions[rule.endpoint])(**view_args)\n",
            "  File \"/content/localGPT/run_localGPT_API.py\", line 176, in prompt_route\n",
            "    res = QA(user_prompt)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 140, in __call__\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/base.py\", line 134, in __call__\n",
            "    self._call(inputs, run_manager=run_manager)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\", line 119, in _call\n",
            "    docs = self._get_docs(question)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/chains/retrieval_qa/base.py\", line 181, in _get_docs\n",
            "    return self.retriever.get_relevant_documents(question)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/base.py\", line 376, in get_relevant_documents\n",
            "    docs = self.vectorstore.similarity_search(query, **self.search_kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\", line 182, in similarity_search\n",
            "    docs_and_scores = self.similarity_search_with_score(query, k, filter=filter)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\", line 230, in similarity_search_with_score\n",
            "    results = self.__query_collection(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/utils.py\", line 53, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/langchain/vectorstores/chroma.py\", line 121, in __query_collection\n",
            "    return self._collection.query(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/api/models/Collection.py\", line 219, in query\n",
            "    return self._client._query(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/api/local.py\", line 408, in _query\n",
            "    uuids, distances = self._db.get_nearest_neighbors(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/db/clickhouse.py\", line 583, in get_nearest_neighbors\n",
            "    uuids, distances = index.get_nearest_neighbors(embeddings, n_results, ids)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/chromadb/db/index/hnswlib.py\", line 230, in get_nearest_neighbors\n",
            "    raise NoIndexException(\n",
            "chromadb.errors.NoIndexException: Index not found, please create an instance before querying\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:47:52] \"\u001b[35m\u001b[1mPOST /api/prompt_route HTTP/1.1\u001b[0m\" 500 -\n",
            "500\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:52] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:52] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:53] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:53] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "WARNING:chromadb:Using embedded DuckDB with persistence: data will be stored in: /content/localGPT/DB\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:47:57] \"GET /api/run_ingest HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:47:57] \"POST / HTTP/1.1\" 200 -\n",
            "User Prompt: what is GCMS?\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:48:19] \"POST /api/prompt_route HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:48:19] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:48:20] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:48:20] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:48:20] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "User Prompt: how to clean EI source?\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:49:14] \"POST /api/prompt_route HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:49:14] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:49:14] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:49:14] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:49:14] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "User Prompt: how to clean EI HES?\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:50:59] \"POST /api/prompt_route HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:50:59] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:50:59] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:50:59] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:50:59] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "User Prompt: how to clean EI HES?\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:52:19] \"POST /api/prompt_route HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:52:19] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:52:20] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:52:20] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:52:20] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "User Prompt: how to maintain the MSD ?\n",
            "INFO:werkzeug:127.0.0.1 - - [07/Aug/2023 02:53:29] \"POST /api/prompt_route HTTP/1.1\" 200 -\n",
            "200\n",
            "127.0.0.1 - - [07/Aug/2023 02:53:29] \"POST / HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:53:29] \"GET /static/dependencies/jquery/3.6.0/jquery.min.js HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:53:29] \"GET /static/dependencies/bootstrap-5.1.3-dist/css/bootstrap.min.css HTTP/1.1\" 200 -\n",
            "127.0.0.1 - - [07/Aug/2023 02:53:29] \"GET /static/dependencies/bootstrap-5.1.3-dist/js/bootstrap.min.js HTTP/1.1\" 200 -\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-6dd94a3721c8>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'python run_localGPT_API.py --device_type cuda & python localGPTUI/localGPTUI.py'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_shell.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     97\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'also_return_output'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_system_commands\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_system_compat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpip_warn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_system_compat\u001b[0;34m(shell, cmd, also_return_output)\u001b[0m\n\u001b[1;32m    451\u001b[0m   \u001b[0;31m# is expected to call this function, thus adding one level of nesting to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m   \u001b[0;31m# stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 453\u001b[0;31m   result = _run_command(\n\u001b[0m\u001b[1;32m    454\u001b[0m       \u001b[0mshell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_run_command\u001b[0;34m(cmd, clear_streamed_output)\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_pty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0m_monitor_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_stdin_widget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_monitor_process\u001b[0;34m(parent_pty, epoll, p, cmd, update_stdin_widget)\u001b[0m\n\u001b[1;32m    231\u001b[0m   \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 233\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_poll_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparent_pty\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    234\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_poll_process\u001b[0;34m(parent_pty, epoll, p, cmd, decoder, state)\u001b[0m\n\u001b[1;32m    279\u001b[0m   \u001b[0moutput_available\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 281\u001b[0;31m   \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mepoll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m   \u001b[0minput_events\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "!python run_localGPT_API.py --device_type cuda & python localGPTUI/localGPTUI.py"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN1zZEybpG04wGKFhIRcXkQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}